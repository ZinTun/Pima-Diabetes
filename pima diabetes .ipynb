{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Diabetes\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pima-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_corr(df, size = 11):\n",
    "    corr = df.corr()\n",
    "    fig , ax = plt.subplots(figsize = (size,size))\n",
    "    ax.matshow(corr)\n",
    "    corr\n",
    "    plt.xticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)),corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete one of the correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['skin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_corr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mold data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all the cols are in numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 'Diabetes' column into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_map = {True : 1, False : 0}\n",
    "df['diabetes'] = df['diabetes'].map(diabetes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = len(df)\n",
    "num_true = len(df.loc[df['diabetes']==1])\n",
    "num_false = len(df.loc[df['diabetes']==0])\n",
    "\n",
    "print(\"Number of True cases:  {0} ({1:2.2f}%)\".format(num_true, (num_true/num_obs) * 100))\n",
    "print(\"Number of False cases: {0} ({1:2.2f}%)\".format(num_false, (num_false/num_obs) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data \n",
    "\n",
    "70% for training, 30% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "feature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\n",
    "predicted_class_names = ['diabetes']\n",
    "\n",
    "X = df[feature_col_names].values\n",
    "y = df[predicted_class_names].values\n",
    "\n",
    "split_test_size = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying predicted value was split correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original True  : {0} ({1:0.2f}%)\".format(len(df.loc[df['diabetes'] == 1]), (len(df.loc[df['diabetes'] == 1])/len(df.index)) * 100.0))\n",
    "print(\"Original False : {0} ({1:0.2f}%)\".format(len(df.loc[df['diabetes'] == 0]), (len(df.loc[df['diabetes'] == 0])/len(df.index)) * 100.0))\n",
    "print(\"\")\n",
    "print(\"Training True  : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train) * 100.0)))\n",
    "print(\"Training False : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train) * 100.0)))\n",
    "print(\"\")\n",
    "print(\"Test True      : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test) * 100.0)))\n",
    "print(\"Test False     : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test) * 100.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-split Data Preparation\n",
    "#### Hidden Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# rows in dataframe {0}\".format(len(df)))\n",
    "print(\"# rows missing glucose_conc: {0}\".format(len(df.loc[df['glucose_conc'] == 0])))\n",
    "print(\"# rows missing diastolic_bp: {0}\".format(len(df.loc[df['diastolic_bp'] == 0])))\n",
    "print(\"# rows missing thickness: {0}\".format(len(df.loc[df['thickness'] == 0])))\n",
    "print(\"# rows missing insulin: {0}\".format(len(df.loc[df['insulin'] == 0])))\n",
    "print(\"# rows missing bmi: {0}\".format(len(df.loc[df['bmi'] == 0])))\n",
    "print(\"# rows missing diab_pred: {0}\".format(len(df.loc[df['diab_pred'] == 0])))\n",
    "print(\"# rows missing age: {0}\".format(len(df.loc[df['age'] == 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#Impute with mean all 0 readings\n",
    "fill_0 = Imputer(missing_values=0, strategy=\"mean\", axis=0)\n",
    "\n",
    "X_train = fill_0.fit_transform(X_train)\n",
    "X_test = fill_0.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Initial Algorithm - Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "nb_predict_train = nb_model.predict(X_train)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, nb_predict_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predict_test = nb_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test,nb_predict_test)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Metrix\")\n",
    "\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test,nb_predict_test)))\n",
    "print()\n",
    "print(\"{0}\".format(metrics.classification_report(y_test,nb_predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42) \n",
    "rf_model.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict_train = rf_model.predict(X_train)\n",
    "\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_train,rf_predict_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict_test = rf_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, rf_predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, rf_predict_test) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, rf_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model =LogisticRegression(C=0.7, random_state=42)\n",
    "lr_model.fit(X_train, y_train.ravel())\n",
    "lr_predict_test = lr_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print(metrics.confusion_matrix(y_test, lr_predict_test) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5\n",
    "C_inc = 0.1\n",
    "\n",
    "C_values, recall_scores = [], []\n",
    "\n",
    "C_val = C_start\n",
    "best_recall_score = 0\n",
    "while (C_val < C_end):\n",
    "    C_values.append(C_val)\n",
    "    lr_model_loop = LogisticRegression(C=C_val, random_state=42)\n",
    "    lr_model_loop.fit(X_train, y_train.ravel())\n",
    "    lr_predict_loop_test = lr_model_loop.predict(X_test)\n",
    "    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)\n",
    "    recall_scores.append(recall_score)\n",
    "    if (recall_score > best_recall_score):\n",
    "        best_recall_score = recall_score\n",
    "        best_lr_predict_test = lr_predict_loop_test\n",
    "        \n",
    "    C_val = C_val + C_inc\n",
    "\n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"1st max value of {0:.3f} occured at C={1:.3f}\".format(best_recall_score, best_score_C_val))\n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values, recall_scores, \"-\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"recall score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic regression with class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5\n",
    "C_inc = 0.1\n",
    "\n",
    "C_values, recall_scores = [], []\n",
    "\n",
    "C_val = C_start\n",
    "best_recall_score = 0\n",
    "while (C_val < C_end):\n",
    "    C_values.append(C_val)\n",
    "    lr_model_loop = LogisticRegression(C=C_val, class_weight=\"balanced\", random_state=42)\n",
    "    lr_model_loop.fit(X_train, y_train.ravel())\n",
    "    lr_predict_loop_test = lr_model_loop.predict(X_test)\n",
    "    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)\n",
    "    recall_scores.append(recall_score)\n",
    "    if (recall_score > best_recall_score):\n",
    "        best_recall_score = recall_score\n",
    "        best_lr_predict_test = lr_predict_loop_test\n",
    "        \n",
    "    C_val = C_val + C_inc\n",
    "\n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"1st max value of {0:.3f} occured at C={1:.3f}\".format(best_recall_score, best_score_C_val))\n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values, recall_scores, \"-\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"recall score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model =LogisticRegression( class_weight=\"balanced\", C=best_score_C_val, random_state=42)\n",
    "lr_model.fit(X_train, y_train.ravel())\n",
    "lr_predict_test = lr_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print(metrics.confusion_matrix(y_test, lr_predict_test) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_predict_test))\n",
    "print(metrics.recall_score(y_test, lr_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs=3, cv=10, refit=False, class_weight=\"balanced\")  # set number of jobs to -1 which uses all cores to parallelize\n",
    "lr_cv_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_predict_test = lr_cv_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_cv_predict_test)))\n",
    "print(metrics.confusion_matrix(y_test, lr_cv_predict_test) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_cv_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using your trained Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib  \n",
    "joblib.dump(lr_cv_model, \"./pima-trained-model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_model = joblib.load(\"./pima-trained-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
